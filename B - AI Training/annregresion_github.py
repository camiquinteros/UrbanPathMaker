# -*- coding: utf-8 -*-
"""ANNRegresion_GitHub.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W1g5Lw-1i8E-YB-5KH0ovz452Zgm1ECW
"""

# Import standard Libraries
import pandas as pd
import seaborn as sns
import altair as alt
import matplotlib.pyplot as plt
import tensorflow as tf


sns.set(rc={'figure.figsize':(10,10)})
print("imports ok")

from google.colab import drive
drive.mount('/content/gdrive')

path = '' #please enter path to your CSV file

data = pd.read_csv(path)
pd.options.display.max_columns = None

print(data.info())

for colname, col in data.iteritems():
  print(colname, "min_val", col.min(), "max_val", col.max())

data1 = data.iloc[:,1:]

print(data1.info())

#declare features
X = data.iloc[:,3:9]
print(X.info())

# Load and instantiate a StandardSclaer 
from sklearn.preprocessing import StandardScaler
scalerX = StandardScaler()

# Apply the scaler to our X-features
X_scaled = scalerX.fit_transform(X)

print(X_scaled.shape)

#declare regression target
y = data.loc[:,"pedestrian_total"].to_numpy()


y = y.reshape(-1, 1)

from sklearn.preprocessing import MinMaxScaler
scalerY = MinMaxScaler()


#In this case it makes sense to use MinMax scaling because the wage seems like a relative range
# Apply the scaler to our Y-features
y_scaled = scalerY.fit_transform(y)

print(y_scaled.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size = 0.2, random_state = 47)

print("TRAIN", "input", X_train.shape, "output", y_train.shape)
print("TEST", "input", X_test.shape, "output", y_test.shape)

# Instantiate a sequential model
model = tf.keras.models.Sequential()
n_cols = X_scaled.shape[1]  

# Add 2 dense layers of 50 and 32 neurons each
model.add(tf.keras.layers.Dense(128, input_shape=(n_cols,), activation='relu'))
model.add(tf.keras.layers.Dense(64, activation='relu'))
  
# Add a dense layer with 1 value output
model.add(tf.keras.layers.Dense(1, activation= "sigmoid"))
  
# Compile your model 
model.compile(optimizer = "adam", loss = "mean_squared_error")

model.summary()

# Fit your model to the training data for 200 epochs
#we assign this to history variable so we can plot the training data
history = model.fit(X_train,y_train,epochs=50, validation_split=0.2)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('loss function')
plt.ylabel('mse')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

modelname='.h5' #name your model with an .h5 extension
model.save(modelname)

#scaling to pickle
from sklearn.externals import joblib
scalerX_filename = "scalerX1.save"
joblib.dump(scalerX, scalerX_filename)

scalerY_filename = "scalerY1.save"
joblib.dump(scalerY, scalerY_filename)